---
k3s_version: {{ k3s_version }}
# this is the user that has ssh access to these machines
ansible_user: ubuntu
systemd_dir: /etc/systemd/system

# Set your timezone
system_timezone: Europe/Rome

# interface which will be used for flannel
flannel_iface: eth0

# uncomment calico_iface to use tigera operator/calico cni instead of flannel https://docs.tigera.io/calico/latest/about
# calico_iface: "eth0"
calico_ebpf: false # use eBPF dataplane instead of iptables
calico_tag: {{ k8s_cluster_pod_network_version }}

# uncomment cilium_iface to use cilium cni instead of flannel or calico
# cilium_iface: "eth0"
cilium_mode: native # native when nodes on same subnet or using bgp, else set routed
cilium_tag: v1.16.1
cilium_hubble: true # enable hubble observability relay and ui

# if using calico or cilium, you may specify the cluster pod cidr pool
cluster_cidr: {{ k8s_cluster_pod_network }}

# enable cilium bgp control plane for lb services and pod cidrs. disables metallb.
cilium_bgp: false

# bgp parameters for cilium cni. only active when cilium_iface is defined and cilium_bgp is true.
cilium_bgp_my_asn: "64513"
cilium_bgp_peer_asn: "64512"
cilium_bgp_peer_address: 192.168.1.1
cilium_bgp_lb_cidr: 172.16.1.0/24 # cidr for cilium loadbalancer ipam

# enable kube-vip ARP broadcasts
kube_vip_arp: true

# enable kube-vip BGP peering
kube_vip_bgp: false

# bgp parameters for kube-vip
kube_vip_bgp_routerid: "127.0.0.1"  # Defines the router ID for the BGP server
kube_vip_bgp_as: "64513"  # Defines the AS for the BGP server
kube_vip_bgp_peeraddress: "192.168.1.1"  # Defines the address for the BGP peer
kube_vip_bgp_peeras: "64512"  # Defines the AS for the BGP peer

# apiserver_endpoint is virtual ip-address which will be configured on each master
#apiserver_endpoint: 192.168.1.222
api_endpoint: "{{ hostvars[groups['server'][0]]['ansible_host'] | default(groups['server'][0]) }}"

# token is required  masters can talk together securely
# this token should be alpha numeric only
token: "{{ k3s_token }}"

cluster_context: k3s-homelab

# Disable the taint manually by setting: k3s_master_taint = false
k3s_master_taint: "{{ true if groups['node'] | default([]) | length >= 1 else false }}"

# image tag for kube-vip
kube_vip_tag_version: v0.8.2

# tag for kube-vip-cloud-provider manifest
# kube_vip_cloud_provider_tag_version: "main"

# kube-vip ip range for load balancer
# (uncomment to use kube-vip for services instead of MetalLB)
# kube_vip_lb_ip_range: "192.168.30.80-192.168.30.90"

# metallb type frr or native
metal_lb_type: native

# metallb mode layer2 or bgp
metal_lb_mode: layer2

# bgp options
# metal_lb_bgp_my_asn: "64513"
# metal_lb_bgp_peer_asn: "64512"
# metal_lb_bgp_peer_address: "192.168.30.1"

# image tag for metal lb
metal_lb_speaker_tag_version: v0.14.8
metal_lb_controller_tag_version: v0.14.8

# metallb ip range for load balancer
metal_lb_ip_range: 192.168.1.80-192.168.1.90

# Only enable if your nodes are proxmox LXC nodes, make sure to configure your proxmox nodes
# in your hosts.ini file.
# Please read https://gist.github.com/triangletodd/02f595cd4c0dc9aac5f7763ca2264185 before using this.
# Most notably, your containers must be privileged, and must not have nesting set to true.
# Please note this script disables most of the security of lxc containers, with the trade off being that lxc
# containers are significantly more resource efficient compared to full VMs.
# Mixing and matching VMs and lxc containers is not supported, ymmv if you want to do this.
# I would only really recommend using this if you have particularly low powered proxmox nodes where the overhead of
# VMs would use a significant portion of your available resources.
proxmox_lxc_configure: false
# the user that you would use to ssh into the host, for example if you run ssh some-user@my-proxmox-host,
# set this value to some-user
proxmox_lxc_ssh_user: root
# the unique proxmox ids for all of the containers in the cluster, both worker and master nodes
proxmox_lxc_ct_ids:
  - 200
  - 201
  - 202
  - 203
  - 204

